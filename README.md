

# Ensemble Models

Before going deep into ensemble models, let us first understand a few important foundational concepts.

## Bias and Variance

Bias measures how far a model’s predictions are from the true values.
- High bias → The model is too simple and fails to capture the underlying patterns in the data. This usually leads to underfitting.
- Low bias → The model is flexible enough to represent the data more accurately.

Variance refers to how sensitive a model is to changes in the training data.
- High variance → The model is too complex and attempts to fit every data point in the training set.
- As a result, even small changes in the training data can lead to large changes in predictions, which typically causes overfitting.

## Underfitting and Overfitting

Underfitting occurs when a model is too simple to capture the underlying structure of the data.
- Characterized by high bias and low variance
- Performs poorly on both training and test datasets

Overfitting occurs when a model learns noise and unnecessary details from the training data instead of the true pattern.
- Characterized by low bias and high variance
- Performs very well on training data but poorly on unseen data

For a model to be ideal, it should have low bias and low variance. However, in practice, achieving both simultaneously is extremely difficult. This trade-off is one of the key reasons why ensemble models are used.

## Ensemble Learning

Instead of relying on a single model, ensemble learning builds multiple models and combines their predictions.

Some models in the ensemble may underfit, while others may overfit. However, when their outputs are combined—typically through averaging (regression) or voting (classification)—the overall performance improves. This aggregation helps reduce variance and can partially compensate for bias, leading to better generalization than any individual model.

In simple terms, an ensemble is a collection of models working together.

Each model in an ensemble should be reasonably good, meaning it should perform better than random guessing (a naive or baseline model). If a model consistently performs worse than the baseline, including it in the ensemble can degrade performance instead of improving it.

Types of Ensemble Methods

There are two widely used approaches to ensemble learning:

1. Bagging (Bootstrap Aggregating)
   - Models are trained independently and in parallel on different bootstrapped samples of the training data.
   - The final prediction is obtained by averaging (for regression) or majority voting (for classification).
   - A well-known algorithm based on bagging is Random Forest.
     <p align="center"> <img src="Images/bagging.webp" alt="Bagging" width="50%"/> </p>

2. Boosting
   - Models are trained sequentially, where each new model focuses on correcting the errors made by previous models.
   - More weight is given to misclassified or poorly predicted data points.
   - Common boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost.
      <p align="center"> <img src="Images/boosting.webp" alt="Boosting" width="50%"/> </p>

## Bagging

In bagging, multiple bootstrap samples are created from the original training dataset.
A bootstrap sample is generated by randomly sampling the training data with replacement.

Each bootstrap sample typically contains 40%–70% of the original dataset, with some data points repeated and others left out.This sampling process is called **bootstrapping**.

Using these different bootstrap samples, multiple models are trained independently. One of the most popular algorithms built using this approach is Random Forest, where each model is a decision tree trained on a different bootstrap sample with same set of tree parameters.


### Random Forest

Random Forest is one of the most widely used bagging algorithms.
- First, multiple bootstrap samples are created from the training dataset.
- Using these samples, multiple decision trees are trained with the same set of parameters, but each tree sees a different subset of the data, which introduces diversity.
- The final prediction is obtained through aggregation:
     - Voting for classification
     - Averaging for regression
 
Key concept:
- At each split in a tree, only a random subset of features is considered.
- Among these features, the tree selects the most informative feature based on metrics such as Gini index or information gain.
- This mechanism ensures that trees are diverse and do not rely on the same dominant features.

<p align="center"> <img src="Images/rf.webp" alt="RandomForest" width="50%"/> </p>

Advantages of Random Forest: 
- Each tree is independent, allowing parallel training, which speeds up computation.
- By considering different subsets of features, Random Forest reduces the curse of dimensionality.
- Since the final output is an aggregate of all trees, the model is more stable. Changes in a few data points usually affect only some trees, not the entire forest.


Random Forest Metric: Out-of-Bag (OOB) Error
- OOB error is the mean prediction error on each training sample, considering only the trees that did not include that sample during training.
- It serves as an internal cross-validation, providing an unbiased estimate of model performance without needing a separate validation set.
